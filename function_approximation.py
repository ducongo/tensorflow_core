# -*- coding: utf-8 -*-
"""
Created on Mon Feb 17 16:45:14 2020

@author: parfa
"""

# -*- coding: utf-8 -*-
"""tensorflwo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vbFASeZuL9211VqTs-EJDyTG1FC1geiU
"""

# Import the needed libraries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import time



def function(x,y):
    return np.cos(x + (6 * 0.35 * y)) + (2 * 0.35 * x * y)

def validationData(n):
    return np.random.rand(n**2,2) * 2 - 1

def data(n):
    uniformRange = np.linspace(-1,1,n)
    grid = list()
    for i in range(0,n):
        for j in range(0,n):
            grid.append([uniformRange[i],uniformRange[j]])
    return np.array(grid)

def create_train_model(train, test,parameters):
    # Reset the graph
    tf.reset_default_graph()

    # Placeholders for input and output data
    X = tf.placeholder(shape=(None, 2), dtype=tf.float64, name='X')
    y = tf.placeholder(shape=(None, 1), dtype=tf.float64, name='y')

    # X = tf.placeholder("float", [None, 2])
    # Y = tf.placeholder("float", [None, 1])

    # Variables for two group of weights between the three layers of the network
    W1 = tf.Variable(np.random.rand(2, parameters['hiddenLayer_size']), dtype=tf.float64)
    W2 = tf.Variable(np.random.rand(parameters['hiddenLayer_size'], 1), dtype=tf.float64)

    # Create the neural net graph
    A1 = tf.nn.tanh(tf.nn.sigmoid(tf.matmul(X, W1)))
    y_est = tf.matmul(A1, W2)

    # Define a loss function
    loss = tf.reduce_sum(tf.square(y_est - y))

    optimizers = []
    optimizers.append(tf.train.GradientDescentOptimizer(parameters['learningRate']).minimize(loss))
    optimizers.append(tf.train.MomentumOptimizer(parameters['learningRate'],0.9).minimize(loss))
    optimizers.append(tf.train.RMSPropOptimizer(learning_rate=parameters['learningRate']).minimize(loss))

    predict_op = y_est

    trX, trY, teX, teY = train['X'], train['y'], test['X'], test['y']

    vX = np.array([])
    vY = np.array([])

    if (parameters['earlyStop']):
        vX = validationData(10)
        vY = function(vX[:,0:],vX[:,1:])
        vError = np.inf

    trainResults = []
    testResults = []
    validationResults = []

    # Go through num_iters iterations
    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        vFail = 0
        for i in range(parameters['epochs']):
            for start, end in zip(range(0, 90, 10), range(10, 100, 10)):
                sess.run(optimizers[parameters['opt_inex']], feed_dict={X: train['X'][start:end], y: train['y'][start:end]})
            if parameters['record']:
                trResults.append(sess.run(tf.losses.mean_squared_error(trY, sess.run(predict_op, feed_dict={X: trX}))))
                teResults.append(sess.run(tf.losses.mean_squared_error(teY, sess.run(predict_op, feed_dict={X: teX}))))
                vResults.append(sess.run(tf.losses.mean_squared_error(vY, sess.run(predict_op, feed_dict={X: vX}))))
            if parameters['earlyStop']:
                error = sess.run(tf.losses.mean_squared_error(vY, sess.run(predict_op, feed_dict={X: vX})))
                #print(error)
                if (error >= vError):
                    vFail += 1 
                else:
                    vError = error
                if (vFail >= 10):
                    print("Validation early stopping")
                    break
            if i % (parameters['epochs']/25) == 0:
                error = sess.run(tf.losses.mean_squared_error(trY, sess.run(predict_op, feed_dict={X: trX})))
                #print(error
                if (error < parameters['threshold']):
                    print("converged below tolerance")
                    break
        print(i, sess.run(tf.losses.mean_squared_error(teY, sess.run(predict_op, feed_dict={X: teX}))))
        #done so do the contour
        if parameters['verbose']['plotResult']:
            contourSize = parameters['verbose']['contourSize']
            contourInputs = np.matrix(data(contourSize))
            Xval = contourInputs[:,0]
            Yval = contourInputs[:,1]
            Z = sess.run(predict_op, feed_dict={X: contourInputs}).reshape(contourSize,contourSize)
            Xval,Yval = np.meshgrid(np.linspace(-1,1,contourSize),np.linspace(-1,1,contourSize))
            Zans = np.cos(Xval + 6 * 0.35 * Yval) + 2 * 0.35 * np.multiply(Xval,Yval)
            plt.contour(Yval, Xval,Z,colors=parameters['verbose']['plotColourIndex'])
    if (parameters['record']):
        return [trResults, teResults, vResults]



contourColours= ['red','blue','green']
hiddenLayers= [2,8,50]
parameters = {
    'hiddenLayer_size': hiddenLayers[0],
    'epochs': 50000,
    'learningRate': 0.02,
    'threshold': 0.02,
    'opt_inex': 0,
    'earlyStop': False,
    'record': False,
    'verbose':{
        'plotResult': True,
        'plotColourIndex': contourColours[0],
        'contourSize': 100
    }
}

train = data(10)
test = data(9)
train_data = {
    'X': train,
    'y': function(train[:,0:1],train[:,1:])
}
test_data = {
    'X': test,
    'y': function(test[:,0:1],test[:,1:])
}
for i in range(0,3):
    parameters['verbose']['plotColourIndex'] = contourColours[i]
    create_train_model(train_data, test_data, parameters)